{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prereqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tinydb ollama pandas pandera bokeh nltk gnews wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# db setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinydb import TinyDB\n",
    "from pandas import DataFrame\n",
    "\n",
    "def save_to_db(df, db_path, table_name):\n",
    "    db = TinyDB(db_path)\n",
    "    table = db.table(table_name)\n",
    "    table.truncate() # clear the table\n",
    "    for i, row in df.iterrows():\n",
    "        table.insert(row.to_dict())\n",
    "    db.close()\n",
    "    print(f\"Saved {len(df)} rows to {db_path}/{table_name}\")\n",
    "    return df\n",
    "\n",
    "def load_from_db(db_path, table_name):\n",
    "    db = TinyDB(db_path)\n",
    "    table = db.table(table_name)\n",
    "    df = DataFrame(table.all())\n",
    "    db.close()\n",
    "    print(f\"Loaded {len(df)} rows from {db_path}/{table_name}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llm setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "# to help find which llm to use:\n",
    "# ollama.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the bot personas\n",
    "\n",
    "news_bot = ollama.Client(host='http://localhost:11434')\n",
    "headline_bot_init = [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': 'You are a journalist writing a news headline. Include only content, no explanation. Include subltle random biases and opinions. Do not ask follow-up questions or include annotations or parenthases.',\n",
    "        },\n",
    "]\n",
    "description_bot_init = [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': 'You are a journalist writing a news description based off of a headline. Include only content, no explanation. Include subltle random biases and opinions. Do not ask follow-up questions or include annotations or parenthases.',\n",
    "        },\n",
    "]\n",
    "\n",
    "\n",
    "def ask(question, context=[{'role':'system', 'content':'You are a helpful knowledge sharer'}]):\n",
    "    response = news_bot.chat(model='llama3.1', messages=[\n",
    "        *context,\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': question,\n",
    "        },\n",
    "        ])\n",
    "    \n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to generate test news headlines and descriptions\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def generate_news_headline():\n",
    "    return ask('generate a single random news headline?', headline_bot_init)\n",
    "\n",
    "def generate_news_description(headline:str):\n",
    "    return ask(f'generate a single random news story based on the headline \"{headline}\"?', description_bot_init)\n",
    "\n",
    "def generate_news():\n",
    "    headline = generate_news_headline()\n",
    "    description = generate_news_description(headline)\n",
    "    date = datetime.now().isoformat()\n",
    "    return headline, description, date\n",
    "\n",
    "def generate_news_batch(n):\n",
    "    return [generate_news() for _ in range(n)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get news headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnews import GNews\n",
    "\n",
    "gnews = GNews(language='en', country='US', period='14d')\n",
    "gnews.max_results = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data modeling and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame, concat, to_datetime\n",
    "from pandera import Column, DataFrameSchema, Index, String, Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_schema = DataFrameSchema({\n",
    "    'headline': Column(String, nullable=False),\n",
    "    'description': Column(String, nullable=False),\n",
    "    'url': Column(String, nullable=False),\n",
    "    'published date': Column(String, nullable=False),\n",
    "    'publisher': Column(Object, nullable=False),\n",
    "},\n",
    "    index=Index(int),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsGatherer:\n",
    "    def __init__(self, language='en', country='US', period='7d', max_results=100):\n",
    "        self.gnews = GNews(language=language, country=country, period=period, exclude_websites=[], max_results=max_results, start_date=None, end_date=None)\n",
    "        self.news = DataFrame()\n",
    "        \n",
    "    def get_news(self, keyword=None, top=False, location=None, topic=None, site=None):\n",
    "        \"\"\"\n",
    "        Retrieves news articles based on specified parameters.\n",
    "        Parameters:\n",
    "        - keyword (str): Optional. Retrieves news articles containing the specified keyword.\n",
    "        - top (bool): Optional. If True, retrieves top news articles.\n",
    "        - location (str): Optional. Retrieves news articles from the specified location.\n",
    "        - topic (str): Optional. Retrieves news articles from the specified topic. Valid topics are 'WORLD', 'NATION', 'BUSINESS', 'TECHNOLOGY', 'ENTERTAINMENT', 'SPORTS', 'SCIENCE', 'HEALTH'.\n",
    "        - site (str): Optional. Retrieves news articles from the specified site.\n",
    "        Returns:\n",
    "        - DataFrame: A DataFrame containing the retrieved news articles.\n",
    "        \"\"\"\n",
    "        \n",
    "        # hardcoded topics\n",
    "        topics =  ['WORLD', 'NATION', 'BUSINESS', 'TECHNOLOGY', 'ENTERTAINMENT', 'SPORTS', 'SCIENCE', 'HEALTH']\n",
    "        news_list = []\n",
    "        if keyword:\n",
    "            news_list.append(DataFrame(self.gnews.get_news(keyword)))\n",
    "        if top:\n",
    "            news_list.append(DataFrame(self.gnews.get_top_news()))\n",
    "        if location:\n",
    "            news_list.append(DataFrame(self.gnews.get_news_by_location(location)))\n",
    "        if topic and topic in topics:\n",
    "            topic = topic.upper()\n",
    "            news_list.append(DataFrame(self.gnews.get_news_by_topic(topic)))\n",
    "        if site:\n",
    "            news_list.append(DataFrame(self.gnews.get_news_by_site(site)))\n",
    "            \n",
    "        self.news = concat(news_list)\n",
    "        self.news.rename(columns={'title':'headline'}, inplace=True)\n",
    "        return self.news\n",
    "    \n",
    "    def return_news(self):\n",
    "        return self.news\n",
    "    \n",
    "    def clean_news(self):\n",
    "        self.news = self.news_schema.validate(self.news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsGenerator:\n",
    "    def __init__(self, language='en', generator='ollama', gen_url='http://localhost:11434/api/chat'):\n",
    "        self.language = language\n",
    "        self.generator = generator\n",
    "        self.gen_url = gen_url\n",
    "        self.fake_news = DataFrame()\n",
    "        self.schema = raw_data_schema\n",
    "\n",
    "    def generate_news(self, n):\n",
    "        self.fake_news = DataFrame(generate_news_batch(n), columns=['headline', 'description', 'date'])\n",
    "        self.clean_news()\n",
    "        return self.fake_news\n",
    "    \n",
    "    def clean_news(self):\n",
    "        self.fake_news['published date'] = self.fake_news['date']\n",
    "        self.fake_news['publisher'] = 'Ollama'\n",
    "        self.fake_news['url'] = self.gen_url\n",
    "        self.fake_news = self.schema.validate(self.fake_news)\n",
    "        return self.fake_news\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gathering and generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngen = NewsGenerator()\n",
    "ngen.generate_news(100)\n",
    "ngen.fake_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nget = NewsGatherer()\n",
    "nget.get_news(keyword='space', topic='technology', top=True, site='nasa.gov')\n",
    "nget.get_news(keyword='science', topic='science', top=True, site='home.cern')\n",
    "\n",
    "nget.get_news(topic='WORLD')\n",
    "nget.get_news(topic='NATION')\n",
    "nget.get_news(topic='BUSINESS')\n",
    "nget.get_news(topic='TECHNOLOGY')\n",
    "nget.get_news(topic='ENTERTAINMENT')\n",
    "nget.get_news(topic='SPORTS')\n",
    "nget.get_news(topic='SCIENCE')\n",
    "nget.get_news(topic='HEALTH')\n",
    "\n",
    "nget.get_news(location='US')\n",
    "nget.get_news(location='UK')\n",
    "nget.get_news(location='Canada')\n",
    "nget.get_news(location='Australia')\n",
    "nget.get_news(location='India')\n",
    "nget.get_news(location='China')\n",
    "nget.get_news(location='Russia')\n",
    "nget.get_news(location='Brazil')\n",
    "nget.get_news(location='Mexico')\n",
    "nget.get_news(location='Japan')\n",
    "\n",
    "nget.get_news(site='nasa.gov')\n",
    "nget.get_news(site='home.cern')\n",
    "nget.get_news(site='www.esa.int')\n",
    "nget.get_news(site='www.spacex.com')\n",
    "nget.get_news(site='www.nasa.gov')\n",
    "\n",
    "nget.get_news(site='www.bbc.com')\n",
    "nget.get_news(site='www.cnn.com')\n",
    "nget.get_news(site='www.foxnews.com')\n",
    "nget.get_news(site='www.nytimes.com')\n",
    "nget.get_news(site='www.washingtonpost.com')\n",
    "nget.get_news(site='www.huffpost.com')\n",
    "nget.get_news(site='www.reuters.com')\n",
    "nget.get_news(site='www.npr.org')\n",
    "nget.get_news(site='www.apnews.com')\n",
    "nget.get_news(site='www.aljazeera.com')\n",
    "\n",
    "nget.news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = concat([ngen.fake_news, nget.news])\n",
    "df = nget.news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from string import punctuation\n",
    "\n",
    "# Initialize NLTK components\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sia = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Map POS tag to first character used by WordNetLemmatizer\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # by default, treat as noun\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha() or word in punctuation]\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tags]\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def get_sentiment_scores(text):\n",
    "    return sia.polarity_scores(text)\n",
    "\n",
    "def analyze_sentiment(headline):\n",
    "    sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "    return sentiment_analyzer.polarity_scores(headline)\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove stopwords\n",
    "    text = ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
    "    # Remove publisher title from headline\n",
    "    text = text.split(' - ')[0]\n",
    "    # Retain only first line\n",
    "    text = text.split('\\n')[0]\n",
    "    # Lemmatize words\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "    # Remove punctuation\n",
    "    text = text.replace('[^\\w\\s]', '')\n",
    "    # Remove extra spaces\n",
    "    text = ' '.join(text.split())\n",
    "    # Remove hyphens\n",
    "    text = text.replace('-', ' ')\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_headline'] = df['headline'].apply(clean_text)\n",
    "df['cleaned_description'] = df['description'].apply(clean_text)\n",
    "df['sentiment'] = df['cleaned_description'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "\n",
    "df['publisher name'] = df['publisher'].apply(lambda x: str(x['title']) if isinstance(x, dict) else None)\n",
    "\n",
    "def apply_sentiment_analysis(df, column_name, prefix):\n",
    "    df[f'{prefix}_sentiment'] = df[column_name].apply(analyze_sentiment)\n",
    "    df[f'{prefix}_positive'] = df[f'{prefix}_sentiment'].apply(lambda x: float(x['pos']))\n",
    "    df[f'{prefix}_negative'] = df[f'{prefix}_sentiment'].apply(lambda x: float(x['neg']))\n",
    "    df[f'{prefix}_neutral'] = df[f'{prefix}_sentiment'].apply(lambda x: float(x['neu']))\n",
    "    df[f'{prefix}_compound'] = df[f'{prefix}_sentiment'].apply(lambda x: float(x['compound']))\n",
    "\n",
    "def apply_wordcount(df, column_name, prefix):\n",
    "    df[f'{prefix}_wordcount'] = df[column_name].apply(lambda x: len(x.split()))\n",
    "\n",
    "def apply_preprocessing_and_sentiment_analysis(df, column_name, prefix):\n",
    "    df[f'abstracted_{prefix}'] = df[column_name].apply(preprocess_text)\n",
    "    apply_sentiment_analysis(df, f'abstracted_{prefix}', f'abstracted_{prefix}')\n",
    "    apply_wordcount(df, f'abstracted_{prefix}', f'abstracted_{prefix}')\n",
    "\n",
    "# Apply sentiment analysis and word count for headlines\n",
    "apply_sentiment_analysis(df, 'cleaned_headline', 'headline')\n",
    "apply_wordcount(df, 'cleaned_headline', 'headline')\n",
    "\n",
    "# Apply sentiment analysis and word count for descriptions\n",
    "apply_sentiment_analysis(df, 'cleaned_description', 'description')\n",
    "apply_wordcount(df, 'cleaned_description', 'description')\n",
    "\n",
    "# Apply preprocessing, sentiment analysis, and word count for abstracted headlines\n",
    "apply_preprocessing_and_sentiment_analysis(df, 'cleaned_headline', 'headline')\n",
    "\n",
    "# Apply preprocessing, sentiment analysis, and word count for abstracted descriptions\n",
    "apply_preprocessing_and_sentiment_analysis(df, 'cleaned_description', 'description')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_grammar_analysis(df, column_name, prefix):\n",
    "    df[f'{prefix}_grammar'] = df[column_name].apply(lambda x: pos_tag(word_tokenize(x)))\n",
    "    df[f'{prefix}_noun'] = df[f'{prefix}_grammar'].apply(lambda x: len([word for word, pos in x if pos in ['NN', 'NNS', 'NNP', 'NNPS']]))\n",
    "    df[f'{prefix}_verb'] = df[f'{prefix}_grammar'].apply(lambda x: len([word for word, pos in x if pos in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']]))\n",
    "    df[f'{prefix}_adjective'] = df[f'{prefix}_grammar'].apply(lambda x: len([word for word, pos in x if pos in ['JJ', 'JJR', 'JJS']]))\n",
    "    df[f'{prefix}_adverb'] = df[f'{prefix}_grammar'].apply(lambda x: len([word for word, pos in x if pos in ['RB', 'RBR', 'RBS']]))\n",
    "    df[f'{prefix}_pronoun'] = df[f'{prefix}_grammar'].apply(lambda x: len([word for word, pos in x if pos in ['PRP', 'PRP$', 'WP', 'WP$']]))\n",
    "    df[f'{prefix}_conjunction'] = df[f'{prefix}_grammar'].apply(lambda x: len([word for word, pos in x if pos in ['CC']]))\n",
    "    df[f'{prefix}_preposition'] = df[f'{prefix}_grammar'].apply(lambda x: len([word for word, pos in x if pos in ['IN']]))\n",
    "    df[f'{prefix}_interjection'] = df[f'{prefix}_grammar'].apply(lambda x: len([word for word, pos in x if pos in ['UH']]))\n",
    "    df[f'{prefix}_grammar_count'] = df[f'{prefix}_grammar'].apply(len)\n",
    "\n",
    "apply_grammar_analysis(df, 'cleaned_headline', 'headline')\n",
    "apply_grammar_analysis(df, 'cleaned_description', 'description')\n",
    "apply_grammar_analysis(df, 'abstracted_headline', 'abstracted_headline')\n",
    "apply_grammar_analysis(df, 'abstracted_description', 'abstracted_description')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook, curdoc\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import ColumnDataSource, Whisker\n",
    "from bokeh.models import HoverTool, LinearColorMapper\n",
    "\n",
    "output_notebook()\n",
    "curdoc().theme = 'night_sky'\n",
    "\n",
    "hover = HoverTool(tooltips=[\n",
    "        ('headline', '@headline'),\n",
    "        ('headline_compound', '@headline_compound'),\n",
    "        ('headline_positive', '@headline_positive'),\n",
    "        ('headline_negative', '@headline_negative'),\n",
    "        ('headline_neutral', '@headline_neutral'),\n",
    "        ('headline_wordcount', '@headline_wordcount'),\n",
    "        # ('description', '@description'),\n",
    "        # ('description_compound', '@description_compound'),\n",
    "        # ('description_positive', '@description_positive'),\n",
    "        # ('description_negative', '@description_negative'),\n",
    "        # ('description_neutral', '@description_neutral'),\n",
    "        # ('description_wordcount', '@description_wordcount'),\n",
    "        # ('abstracted_headline', '@abstracted_headline'),\n",
    "        # ('abstracted_headline_compound', '@abstracted_headline_compound'),\n",
    "        # ('abstracted_headline_positive', '@abstracted_headline_positive'),\n",
    "        # ('abstracted_headline_negative', '@abstracted_headline_negative'),\n",
    "        # ('abstracted_headline_neutral', '@abstracted_headline_neutral'),\n",
    "        # ('abstracted_headline_wordcount', '@abstracted_headline_wordcount'),\n",
    "        # ('abstracted_description', '@abstracted_description'),\n",
    "        # ('abstracted_description_compound', '@abstracted_description_compound'),\n",
    "        # ('abstracted_description_positive', '@abstracted_description_positive'),\n",
    "        # ('abstracted_description_negative', '@abstracted_description_negative'),\n",
    "        # ('abstracted_description_neutral', '@abstracted_description_neutral'),\n",
    "        # ('abstracted_description_wordcount', '@abstracted_description_wordcount'),\n",
    "\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_df.plot(kind='kde')\n",
    "# valid_df.plot(kind='hist')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, get_single_color_func\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import pos_tag\n",
    "\n",
    "class GroupedColorFunc(object):\n",
    "    \"\"\"Create a color function object which assigns DIFFERENT SHADES of\n",
    "       specified colors to certain words based on the color to words mapping.\n",
    "\n",
    "       Uses wordcloud.get_single_color_func\n",
    "\n",
    "       Parameters\n",
    "       ----------\n",
    "       color_to_words : dict(str -> list(str))\n",
    "         A dictionary that maps a color to the list of words.\n",
    "\n",
    "       default_color : str\n",
    "         Color that will be assigned to a word that's not a member\n",
    "         of any value from color_to_words.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, color_to_words, default_color):\n",
    "        self.color_func_to_words = [\n",
    "            (get_single_color_func(color), set(words))\n",
    "            for (color, words) in color_to_words.items()]\n",
    "\n",
    "        self.default_color_func = get_single_color_func(default_color)\n",
    "\n",
    "    def get_color_func(self, word):\n",
    "        \"\"\"Returns a single_color_func associated with the word\"\"\"\n",
    "        try:\n",
    "            color_func = next(\n",
    "                color_func for (color_func, words) in self.color_func_to_words\n",
    "                if word in words)\n",
    "        except StopIteration:\n",
    "            color_func = self.default_color_func\n",
    "\n",
    "        return color_func\n",
    "\n",
    "    def __call__(self, word, **kwargs):\n",
    "        return self.get_color_func(word)(word, **kwargs)\n",
    "\n",
    "color_to_pos = {\n",
    "    'blue': ['NN', 'NNS', 'NNP', 'NNPS'],\n",
    "    'green': ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'],\n",
    "    'red': ['JJ', 'JJR', 'JJS'],\n",
    "    'orange': ['RB', 'RBR', 'RBS'],\n",
    "    'purple': ['PRP', 'PRP$', 'WP', 'WP$'],\n",
    "    'yellow': ['CC'],\n",
    "    'pink': ['IN'],\n",
    "    'brown': ['UH'],\n",
    "}\n",
    "\n",
    "words_df = df['cleaned_headline'].str.split(expand=True).stack().reset_index().rename(columns={'level_0': 'index', 0: 'word'})\n",
    "words_df['word'] = words_df['word'].str.replace(\"'s\", \"\")\n",
    "strings_to_replace = ['.', ',', '!', '?', ':', ';', '(', ')', '[', ']', '{', '}', '\"', \"'\", '`']\n",
    "for s in strings_to_replace:\n",
    "    words_df['word'] = words_df['word'].str.replace(s, '')\n",
    "\n",
    "def categorize_words_to_dict(df, column_name):\n",
    "    words_df = df[column_name].str.split(expand=True).stack().reset_index().rename(columns={'level_0': 'index', 0: 'word'})\n",
    "    words_df = words_df[words_df['word'].str.len() > 2]  # Filter out single-letter words\n",
    "    words_df['pos'] = words_df['word'].apply(lambda x: pos_tag([x])[0][1])\n",
    "    return words_df.groupby('pos')['word'].apply(list).to_dict()\n",
    "\n",
    "def calculate_word_colors(df, column_name):\n",
    "    words_dict = categorize_words_to_dict(df, column_name)\n",
    "    color_to_words = {color: words for color, words in color_to_pos.items() if color in words_dict}\n",
    "    return color_to_words\n",
    "\n",
    "def word_cloud(df):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    wordcloud = WordCloud(width=1000, height=750, max_font_size=1000, max_words=500, background_color='darkgrey')\n",
    "    color_to_words = calculate_word_colors(df, 'word')\n",
    "    wordcloud.generate_from_frequencies(frequencies=df['word'].value_counts().to_dict())\n",
    "\n",
    "\n",
    "\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    return plt\n",
    "\n",
    "plot_df = df.copy()\n",
    "plot_df.groupby('publisher name')\n",
    "\n",
    "wordcloud = word_cloud(words_df)\n",
    "wordcloud.savefig('wordcloud.png')\n",
    "wordcloud.show()\n",
    "words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logscale\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "\n",
    "def plot_word_counts(df):\n",
    "    plot_df = df['cleaned_headline'].str.split(expand=True).stack().value_counts().reset_index().rename(columns={'index': 'word', 0: 'count'})\n",
    "    p = figure(x_range=plot_df['word'], title='Word Frequency', toolbar_location=None, tools='', min_width=600, min_height=400)\n",
    "    \n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.ygrid.grid_line_color = None\n",
    "    p.scatter(x='word', y='count', size='count', fill_color='blue', line_color='white', source=plot_df)\n",
    "    p.add_tools(HoverTool(tooltips=[('Word', '@word'), ('Count', '@count')]))\n",
    "\n",
    "    show(p)\n",
    "\n",
    "plot_word_counts(plot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "from bokeh.palettes import Category20c\n",
    "\n",
    "from bokeh.transform import cumsum\n",
    "\n",
    "def pie_cart_wordcount(df):\n",
    "    plot_df = df['word'].str.split(expand=True).stack().value_counts().reset_index().rename(columns={'index': 'word', 0: 'count'})\n",
    "    plot_df['angle'] = plot_df['count']/plot_df['count'].sum() * 2*np.pi\n",
    "    # plot_df['color'] = Category20c[len(plot_df)]\n",
    "\n",
    "    p = figure(height=350, title='Word Frequency', toolbar_location=None, tools='', min_width=600, min_height=400)\n",
    "    p.wedge(x=0, y=1, radius=0.4, start_angle=cumsum('angle', include_zero=True), end_angle=cumsum('angle'), line_color='white', legend_field='word', source=plot_df)\n",
    "    p.axis.axis_label = None\n",
    "    p.axis.visible = False\n",
    "    p.grid.grid_line_color = None\n",
    "\n",
    "    p.add_tools(HoverTool(tooltips=[('Word', '@word'), ('Count', '@count')]))\n",
    "\n",
    "    show(p)\n",
    "\n",
    "pie_cart_wordcount(words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter_sentiment(df):\n",
    "    source = ColumnDataSource(df)\n",
    "    p = figure()\n",
    "    radii = df['headline_compound']\n",
    "    color_mapper = LinearColorMapper(palette='Viridis256', low=min(radii), high=max(radii))\n",
    "    p.scatter(x='headline_positive', y='headline_negative', source=source, fill_color={'field': 'headline_compound', 'transform': color_mapper}, size='headline_wordcount', line_color='black', line_width=0.5)\n",
    "    p.xaxis.axis_label = 'Positivity'\n",
    "    p.yaxis.axis_label = 'Negativity'\n",
    "    p.title.text = 'Word Counts'\n",
    "    p.add_tools(hover)\n",
    "    return p\n",
    "\n",
    "show(plot_scatter_sentiment(plot_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplot(df):\n",
    "    p = figure(\n",
    "        title='Boxplot for Sentiment Analysis of News titles',\n",
    "        x_range=df['headline'].unique(),\n",
    "        x_axis_label='headline',\n",
    "        y_axis_label='Sentiment Score',\n",
    "        min_width=1200,\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "    p.add_tools(hover)\n",
    "\n",
    "    source = ColumnDataSource(df)\n",
    "\n",
    "    headline_whisker = Whisker(source=source, base='headline', upper='headline_positive', lower='headline_negative', line_color='black')\n",
    "    p.add_layout(headline_whisker)\n",
    "\n",
    "    p.vbar(x='headline', top='headline_compound', bottom='headline_neutral', width=0.5, source=source, fill_color='blue', line_color='black')\n",
    "    p.vbar(x='headline', top='headline_neutral', bottom='headline_negative', width=0.5, source=source, fill_color='green', line_color='black')\n",
    "\n",
    "    # p.scatter(x='headline', y='headline_compound', source=source, color='blue', legend_label='Compound')\n",
    "    # p.scatter(x='headline', y='headline_positive', source=source, color='green', legend_label='Positive')\n",
    "    # p.scatter(x='headline', y='headline_negative', source=source, color='red', legend_label='Negative')\n",
    "    # p.scatter(x='headline', y='headline_neutral', source=source, color='gray', legend_label='Neutral')\n",
    "    return p\n",
    "\n",
    "show(boxplot(plot_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(df):\n",
    "    source = ColumnDataSource(df)\n",
    "    p = figure(title='Sentiment Analysis', x_axis_label='Neutral', y_axis_label='Pos/Neg/Compound', min_width=800)\n",
    "    p.add_tools(hover)\n",
    "    p.scatter(x='headline_neutral', y='headline_positive', source=source, color='#14346499', size='abstracted_headline_wordcount', legend_label='Positive')\n",
    "    p.scatter(x='headline_neutral', y='headline_negative', source=source, color='#34146499', size='abstracted_headline_wordcount', legend_label='Negative')\n",
    "    p.scatter(x='headline_neutral', y='headline_compound', source=source, color='#46413499', size='abstracted_headline_wordcount', legend_label='Compound')\n",
    "    return p\n",
    "\n",
    "show(plot_scatter(plot_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sentiment_scatter(df):\n",
    "    p = figure(title='Sentiment Analysis', x_axis_label='Word Count', y_axis_label='Sentiment Score', min_width=800)\n",
    "    p.add_tools(hover)\n",
    "    p.scatter(x='headline_wordcount', y='headline_compound', source=ColumnDataSource(df), size='abstracted_headline_wordcount', color='blue')\n",
    "    return p\n",
    "\n",
    "show(plot_sentiment_scatter(plot_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.layouts import gridplot\n",
    "\n",
    "dfs = [plot_df, df]\n",
    "\n",
    "def plot_all(dfs):\n",
    "    plots = []\n",
    "    for df in dfs:\n",
    "        plots.append(boxplot(df))\n",
    "        plots.append(plot_scatter(df))\n",
    "        plots.append(plot_sentiment_scatter(df))\n",
    "    return gridplot(plots, ncols=3)\n",
    "\n",
    "show(plot_all(dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
